{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-19 06:05:26.526078: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-19 06:05:27.429466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-19 06:05:30.552102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-19 06:05:30.553615: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/opt/ml/level3_nlp_finalproject-nlp-11/T5/results\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#허깅\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name,from_tf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m input_text \u001b[39m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mif there is a line that lies parallel to the horizon it is said to be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39ma legislative act passed by congress is an example of\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m max_token_length \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    492\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    494\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    495\u001b[0m     )\n\u001b[1;32m    496\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:2869\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2867\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_tf2_checkpoint_in_pytorch_model\n\u001b[0;32m-> 2869\u001b[0m     model, loading_info \u001b[39m=\u001b[39m load_tf2_checkpoint_in_pytorch_model(\n\u001b[1;32m   2870\u001b[0m         model, resolved_archive_file, allow_missing_keys\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_loading_info\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m   2871\u001b[0m     )\n\u001b[1;32m   2872\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   2873\u001b[0m     logger\u001b[39m.\u001b[39merror(\n\u001b[1;32m   2874\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLoading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2875\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2876\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m instructions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2877\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_tf_pytorch_utils.py:451\u001b[0m, in \u001b[0;36mload_tf2_checkpoint_in_pytorch_model\u001b[0;34m(pt_model, tf_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m tf_inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m     tf_model(tf_inputs, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)  \u001b[39m# Make sure model is built\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m load_tf_weights(tf_model, tf_checkpoint_path)\n\u001b[1;32m    453\u001b[0m \u001b[39mreturn\u001b[39;00m load_tf2_model_in_pytorch_model(\n\u001b[1;32m    454\u001b[0m     pt_model, tf_model, allow_missing_keys\u001b[39m=\u001b[39mallow_missing_keys, output_loading_info\u001b[39m=\u001b[39moutput_loading_info\n\u001b[1;32m    455\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:880\u001b[0m, in \u001b[0;36mload_tf_weights\u001b[0;34m(model, resolved_archive_file, ignore_mismatched_sizes, _prefix)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    878\u001b[0m     load_function \u001b[39m=\u001b[39m load_tf_weights_from_h5\n\u001b[0;32m--> 880\u001b[0m \u001b[39mreturn\u001b[39;00m load_function(\n\u001b[1;32m    881\u001b[0m     model, resolved_archive_file, ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes, _prefix\u001b[39m=\u001b[39;49m_prefix\n\u001b[1;32m    882\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:889\u001b[0m, in \u001b[0;36mload_tf_weights_from_h5\u001b[0;34m(model, resolved_archive_file, ignore_mismatched_sizes, _prefix)\u001b[0m\n\u001b[1;32m    886\u001b[0m mismatched_layers \u001b[39m=\u001b[39m []\n\u001b[1;32m    888\u001b[0m \u001b[39m# Read the H5 file\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39;49mFile(resolved_archive_file, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m sharded_checkpoint_file:\n\u001b[1;32m    890\u001b[0m     \u001b[39m# Retrieve the name of each layer from the H5 file\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     saved_h5_model_layers_name \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(load_attributes_from_hdf5_group(sharded_checkpoint_file, \u001b[39m\"\u001b[39m\u001b[39mlayer_names\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    893\u001b[0m     \u001b[39m# Find the missing layers from the high level list of layers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[1;32m    232\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_name = \"/opt/ml/level3_nlp_finalproject-nlp-11/T5/results\" #허깅\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "input_text = [\n",
    "    \"if there is a line that lies parallel to the horizon it is said to be\",\n",
    "    'a legislative act passed by congress is an example of'\n",
    "]\n",
    "max_token_length = 64\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\",\n",
    "                   padding=True, max_length=max_token_length)\n",
    "results = model.generate(\n",
    "    **inputs,\n",
    "    max_length=max_token_length,\n",
    "    num_beams=5,\n",
    ")\n",
    "[\n",
    "    tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(korean)) for korean in results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
